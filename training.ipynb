{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle \n",
    "from os.path import join \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# current directory where .pkl files are present (for features extraction)\n",
    "DATA_DIR = join(\"./\")\n",
    "\n",
    "def load_batches_to_dict(amount_to_load=3):\n",
    "    \"\"\"Loads batches (.pkl files) from disc and returns one concatenated dict (features in csv).\n",
    "    amount_to_load specifies the number of batches to load, starting from 1.\"\"\"\n",
    "    if amount_to_load < 1 or amount_to_load > 3:\n",
    "        raise \"amount_to_load is not a valid number! Try a number between 1 and 3.\"\n",
    "\n",
    "    batches_dict = {}  # Initializing\n",
    "\n",
    "    # Replicating Load Data logic\n",
    "    print(\"Loading batch1 ...\")\n",
    "    # path of batch1.pkl ./batch1.pkl\n",
    "    path1 = join(DATA_DIR, \"batch1.pkl\")\n",
    "    # loading in memory\n",
    "    batch1 = pickle.load(open(path1, 'rb'))\n",
    "\n",
    "    #remove batteries that do not reach 80% capacity\n",
    "    del batch1['b1c8']\n",
    "    del batch1['b1c10']\n",
    "    del batch1['b1c12']\n",
    "    del batch1['b1c13']\n",
    "    del batch1['b1c22']\n",
    "    \n",
    "    # updates/replaces the values of dictionary with the new dictionary)\n",
    "    batches_dict.update(batch1)\n",
    "\n",
    "    if amount_to_load > 1:\n",
    "        print(\"Loading batch2 ...\")\n",
    "        path2 = join(DATA_DIR, \"batch2.pkl\")\n",
    "        batch2 = pickle.load(open(path2, 'rb'))\n",
    "\n",
    "        # There are four cells from batch1 that carried into batch2, we'll remove the data from batch2\n",
    "        # and put it with the correct cell from batch1\n",
    "        batch2_keys = ['b2c7', 'b2c8', 'b2c9', 'b2c15', 'b2c16']\n",
    "        batch1_keys = ['b1c0', 'b1c1', 'b1c2', 'b1c3', 'b1c4']\n",
    "        add_len = [662, 981, 1060, 208, 482]\n",
    "\n",
    "        for i, bk in enumerate(batch1_keys):\n",
    "            batch1[bk]['cycle_life'] = batch1[bk]['cycle_life'] + add_len[i]\n",
    "            for j in batch1[bk]['summary'].keys():\n",
    "                if j == 'cycle':\n",
    "                    batch1[bk]['summary'][j] = np.hstack((batch1[bk]['summary'][j], batch2[batch2_keys[i]]['summary'][j] + len(batch1[bk]['summary'][j])))\n",
    "                else:\n",
    "                    batch1[bk]['summary'][j] = np.hstack((batch1[bk]['summary'][j], batch2[batch2_keys[i]]['summary'][j]))\n",
    "            last_cycle = len(batch1[bk]['cycles'].keys())\n",
    "            for j, jk in enumerate(batch2[batch2_keys[i]]['cycles'].keys()):\n",
    "                batch1[bk]['cycles'][str(last_cycle + j)] = batch2[batch2_keys[i]]['cycles'][jk]\n",
    "\n",
    "        del batch2['b2c7']\n",
    "        del batch2['b2c8']\n",
    "        del batch2['b2c9']\n",
    "        del batch2['b2c15']\n",
    "        del batch2['b2c16']\n",
    "\n",
    "        # All keys have to be updated after the reordering.\n",
    "        batches_dict.update(batch1)\n",
    "        batches_dict.update(batch2)\n",
    "\n",
    "    if amount_to_load > 2:\n",
    "        print(\"Loading batch3 ...\")\n",
    "        path3 = join(DATA_DIR, \"batch3.pkl\")\n",
    "        batch3 = pickle.load(open(path3, 'rb'))\n",
    "\n",
    "        # remove noisy channels from batch3\n",
    "        del batch3['b3c37']\n",
    "        del batch3['b3c2']\n",
    "        del batch3['b3c23']\n",
    "        del batch3['b3c32']\n",
    "        del batch3['b3c38']\n",
    "        del batch3['b3c39']\n",
    "\n",
    "        batches_dict.update(batch3)\n",
    "\n",
    "    print(\"Done loading batches\")\n",
    "    return batches_dict\n",
    "\n",
    "\n",
    "def build_feature_df(batch_dict):\n",
    "    \"\"\"Returns a pandas DataFrame with all originally used features out of a loaded batch dict\"\"\"\n",
    "\n",
    "    print(\"Start building features ...\")\n",
    "\n",
    "    from scipy.stats import skew, kurtosis\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # 124 cells (3 batches)\n",
    "    n_cells = len(batch_dict.keys())\n",
    "\n",
    "    ## Initializing feature vectors:\n",
    "    # numpy vector with 124 zeros\n",
    "    cycle_life = np.zeros(n_cells)\n",
    "    # 1. delta_Q_100_10(V)\n",
    "    minimum_dQ_100_10 = np.zeros(n_cells)\n",
    "    variance_dQ_100_10 = np.zeros(n_cells)\n",
    "    skewness_dQ_100_10 = np.zeros(n_cells)\n",
    "    kurtosis_dQ_100_10 = np.zeros(n_cells)\n",
    "\n",
    "    # 2. Discharge capacity fade curve features\n",
    "    slope_lin_fit_2_100 = np.zeros(n_cells)  # Slope of the linear fit to the capacity fade curve, cycles 2 to 100\n",
    "    intercept_lin_fit_2_100 = np.zeros(n_cells)  # Intercept of the linear fit to capavity face curve, cycles 2 to 100\n",
    "    discharge_capacity_2 = np.zeros(n_cells)  # Discharge capacity, cycle 2\n",
    "    diff_discharge_capacity_max_2 = np.zeros(n_cells)  # Difference between max discharge capacity and cycle 2\n",
    "\n",
    "    # 3. Other features\n",
    "    mean_charge_time_2_6 = np.zeros(n_cells)  # Average charge time, cycle 2 to 6\n",
    "    minimum_IR_2_100 = np.zeros(n_cells)  # Minimum internal resistance\n",
    "   \n",
    "    diff_IR_100_2 = np.zeros(n_cells)  # Internal resistance, difference between cycle 100 and cycle 2\n",
    "\n",
    "    # Classifier features\n",
    "    minimum_dQ_5_4 = np.zeros(n_cells)\n",
    "    variance_dQ_5_4 = np.zeros(n_cells)\n",
    "    cycle_550_clf = np.zeros(n_cells)\n",
    "    \n",
    "    # iterate/loop over all cells.\n",
    "    for i, cell in enumerate(batch_dict.values()):\n",
    "        cycle_life[i] = cell['cycle_life'] \n",
    "        # 1. delta_Q_100_10(V)\n",
    "        c10 = cell['cycles']['10']\n",
    "        c100 = cell['cycles']['100']\n",
    "        dQ_100_10 = c100['Qdlin'] - c10['Qdlin']\n",
    "\n",
    "        minimum_dQ_100_10[i] = np.log10(np.abs(np.min(dQ_100_10)))\n",
    "        variance_dQ_100_10[i] = np.log(np.abs(np.var(dQ_100_10)))\n",
    "        skewness_dQ_100_10[i] = np.log(np.abs(skew(dQ_100_10)))\n",
    "        kurtosis_dQ_100_10[i] = np.log(np.abs(kurtosis(dQ_100_10)))\n",
    "\n",
    "        # 2. Discharge capacity fade curve features\n",
    "        # Compute linear fit for cycles 2 to 100:\n",
    "        q = cell['summary']['QD'][1:100].reshape(-1, 1)  # discharge cappacities; q.shape = (99, 1);\n",
    "        X = cell['summary']['cycle'][1:100].reshape(-1, 1)  # Cylce index from 2 to 100; X.shape = (99, 1)\n",
    "        linear_regressor_2_100 = LinearRegression()\n",
    "        linear_regressor_2_100.fit(X, q)\n",
    "\n",
    "        slope_lin_fit_2_100[i] = linear_regressor_2_100.coef_[0]\n",
    "        intercept_lin_fit_2_100[i] = linear_regressor_2_100.intercept_\n",
    "        discharge_capacity_2[i] = q[0][0]\n",
    "        diff_discharge_capacity_max_2[i] = np.max(q) - q[0][0]\n",
    "\n",
    "        # 3. Other features\n",
    "        mean_charge_time_2_6[i] = np.mean(cell['summary']['chargetime'][1:6])\n",
    "        minimum_IR_2_100[i] = np.min(cell['summary']['IR'][1:100])\n",
    "        diff_IR_100_2[i] = cell['summary']['IR'][100] - cell['summary']['IR'][1]\n",
    "\n",
    "        # Classifier features\n",
    "        c4 = cell['cycles']['4']\n",
    "        c5 = cell['cycles']['5']\n",
    "        dQ_5_4 = c5['Qdlin'] - c4['Qdlin']\n",
    "        minimum_dQ_5_4[i] = np.log10(np.abs(np.min(dQ_5_4)))\n",
    "        variance_dQ_5_4[i] = np.log10(np.var(dQ_5_4))\n",
    "        cycle_550_clf[i] = cell['cycle_life'] >= 550\n",
    "\n",
    "    # combining all featues in one big matrix where rows are the cells and colums are the features\n",
    "    # note last two variables below are labels/targets for ML i.e cycle life and cycle_550_clf\n",
    "    features_df = pd.DataFrame({\n",
    "        \"cell_key\": np.array(list(batch_dict.keys())),\n",
    "        \"minimum_dQ_100_10\": minimum_dQ_100_10,\n",
    "        \"variance_dQ_100_10\": variance_dQ_100_10,\n",
    "        \"skewness_dQ_100_10\": skewness_dQ_100_10,\n",
    "        \"kurtosis_dQ_100_10\": kurtosis_dQ_100_10,\n",
    "        \"slope_lin_fit_2_100\": slope_lin_fit_2_100,\n",
    "        \"intercept_lin_fit_2_100\": intercept_lin_fit_2_100,\n",
    "        \"discharge_capacity_2\": discharge_capacity_2,\n",
    "        \"diff_discharge_capacity_max_2\": diff_discharge_capacity_max_2,\n",
    "        \"mean_charge_time_2_6\": mean_charge_time_2_6,\n",
    "        \"minimum_IR_2_100\": minimum_IR_2_100,\n",
    "        \"diff_IR_100_2\": diff_IR_100_2,\n",
    "        \"minimum_dQ_5_4\": minimum_dQ_5_4,\n",
    "        \"variance_dQ_5_4\": variance_dQ_5_4,\n",
    "        \"cycle_life\": cycle_life,\n",
    "        \"cycle_550_clf\": cycle_550_clf\n",
    "    })\n",
    "\n",
    "    print(\"Done building features\")\n",
    "    return features_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading batch1 ...\n",
      "Loading batch2 ...\n",
      "Loading batch3 ...\n",
      "Done loading batches\n",
      "Start building features ...\n",
      "Done building features\n",
      "Saved features to  ./rebuild_features.csv\n"
     ]
    }
   ],
   "source": [
    "# calling function to load from disk\n",
    "all_batches_dict = load_batches_to_dict()\n",
    "# function to build features for ML\n",
    "features_df = build_feature_df(all_batches_dict)\n",
    "\n",
    "# save all features 124 in disk in rebuild_features.csv file\n",
    "save_csv_path = join(DATA_DIR, \"rebuild_features.csv\")\n",
    "features_df.to_csv(save_csv_path, index=False)\n",
    "print(\"Saved features to \", save_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(features, regression_type=\"full\", model=\"regression\"):\n",
    "    # only three versions are allowed.\n",
    "    assert regression_type in [\"full\", \"variance\", \"discharge\"]\n",
    "    \n",
    "    # dictionary to hold the features indices for each model version.\n",
    "    features = {\n",
    "        \"full\": [1, 2, 5, 6, 7, 9, 10, 11],\n",
    "        \"variance\": [2],\n",
    "        \"discharge\": [1, 2, 3, 4, 7, 8]\n",
    "    }\n",
    "    # get the features for the model version (full, variance, discharge)\n",
    "    feature_indices = features[regression_type]\n",
    "    # get all cells with the specified features \n",
    "    model_features = features_df.iloc[:,feature_indices]\n",
    "    # get last two columns (cycle life and classification)\n",
    "    labels = features_df.iloc[:, -2:] \n",
    "    # labels are (cycle life ) for regression other wise (0/1) for classsification\n",
    "    labels = labels.iloc[:, 0] if model == \"regression\" else labels.iloc[:, 1]\n",
    "    \n",
    "    # split data in to train/primary_test/and secondary test\n",
    "    train_cells = np.arange(1,83,2) \n",
    "    val_cells = np.arange(0,84,2)\n",
    "    test_cells = np.arange(84, 124, 1)\n",
    "    \n",
    "    # get cells and their features of each set and convert to numpy for further computations\n",
    "    x_train = np.array(model_features.iloc[train_cells])\n",
    "    x_val = np.array(model_features.iloc[val_cells])\n",
    "    x_test = np.array(model_features.iloc[test_cells])\n",
    "    \n",
    "    # target values or labels for training\n",
    "    y_train = np.array(labels.iloc[train_cells])\n",
    "    y_val = np.array(labels.iloc[val_cells])\n",
    "    y_test = np.array(labels.iloc[test_cells])\n",
    "    \n",
    "    # return 3 sets\n",
    "    return {\"train\": (x_train, y_train), \"val\": (x_val, y_val), \"test\": (x_test, y_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import linear_model\n",
    " \n",
    "def regression(datasets, pca=False, normalize=True, log_target=True,\n",
    "               n_components=5, alpha=1e-4, l1_ratio=0.99, model=\"elastic\"):\n",
    "    # get three sets\n",
    "    x_train, y_train = datasets.get(\"train\")\n",
    "    x_val, y_val = datasets.get(\"val\")\n",
    "    x_test, y_test = datasets.get(\"test\")\n",
    "    \n",
    "    \n",
    "    if pca:\n",
    "        # create pca object with n_components (n_components is the reduced feature dimension)\n",
    "        _pca = PCA(n_components=n_components)\n",
    "        _pca.fit(x_train)\n",
    "        # transfrom data to reduced features\n",
    "        x_train = _pca.transform(x_train)\n",
    "        x_val = _pca.transform(x_val)\n",
    "        x_test = _pca.transform(x_test)\n",
    "    if normalize:\n",
    "        # create normalization object to normalize data\n",
    "        scaler = StandardScaler()\n",
    "        # find normalization params from train set, (mean, std)\n",
    "        scaler.fit(x_train)\n",
    "        # normalize sets (0 mean, 1 std)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_val = scaler.transform(x_val)\n",
    "        x_test = scaler.transform(x_test) \n",
    "\n",
    "\n",
    "    \n",
    "    if model == \"elastic\":\n",
    "        print (\"Elastic net\")\n",
    "        regr = ElasticNet(random_state=4, alpha=alpha, l1_ratio=l1_ratio)\n",
    "    else:\n",
    "        print (\"Lasso net\")\n",
    "        regr = Lasso(alpha=alpha)\n",
    "    # labels/ targets might be converted to log version based on choice\n",
    "    targets = np.log(y_train) if log_target else y_train\n",
    "    # fit regression model\n",
    "    regr.fit(x_train, targets)\n",
    "\n",
    "    # predict values/cycle life for all three sets\n",
    "    pred_train = regr.predict(x_train)\n",
    "    pred_val = regr.predict(x_val)\n",
    "    pred_test = regr.predict(x_test)\n",
    "    \n",
    "    if log_target:\n",
    "        # scale up the preedictions\n",
    "        pred_train, pred_val, pred_test = np.exp(pred_train), np.exp(pred_val), np.exp(pred_test)\n",
    "    \n",
    "    # mean percentage error (same as paper)\n",
    "    error_train = mean_absolute_percentage_error(y_train, pred_train)*100\n",
    "    error_val = mean_absolute_percentage_error(y_val, pred_val)*100\n",
    "    error_test = mean_absolute_percentage_error(y_test, pred_test)*100\n",
    "    \n",
    "    print(f\"Regression Error batch 3 (test (secondary)):  {error_test}%\" )\n",
    "    print(f\"Regression Error (Train):, {error_train}%\")\n",
    "    print(f\"Regression Error (validation (primary) test): {error_val}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full\n",
      "Elastic net\n",
      "Regression Error batch 3 (test (secondary)):  16.48701158264002%\n",
      "Regression Error (Train):, 11.014193993605865%\n",
      "Regression Error (validation (primary) test): 22.87890922366772%\n",
      "\n",
      "Discharge\n",
      "Elastic net\n",
      "Regression Error batch 3 (test (secondary)):  16.647625827135297%\n",
      "Regression Error (Train):, 11.461057782933471%\n",
      "Regression Error (validation (primary) test): 18.663078336586363%\n",
      "\n",
      "Variance\n",
      "Elastic net\n",
      "Regression Error batch 3 (test (secondary)):  11.689134915392648%\n",
      "Regression Error (Train):, 16.069316810065658%\n",
      "Regression Error (validation (primary) test): 16.928284800729614%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # skip this cell if features are already build\n",
    "    # read features from CSV file\n",
    "    import pandas as pd\n",
    "    features_df = pd.read_csv(\"./rebuild_features.csv\")\n",
    "    print (\"Full\")\n",
    "    features = train_val_split(features_df, regression_type=\"full\")\n",
    "    regression(features, pca=False, normalize=False, n_components=3, alpha=0.0005,\n",
    "               l1_ratio=1, log_target=False, model=\"elastic\")\n",
    "\n",
    "    print (\"\\nDischarge\")\n",
    "    features = train_val_split(features_df, regression_type=\"discharge\")\n",
    "    regression(features, pca=False, normalize=False, n_components=3, alpha=0.0001,\n",
    "               l1_ratio=1, log_target=True, model=\"elastic\")\n",
    "\n",
    "    print (\"\\nVariance\")\n",
    "    features = train_val_split(features_df, regression_type=\"variance\")\n",
    "    regression(features, pca=False, normalize=False, n_components=3, alpha=0.001,\n",
    "               l1_ratio=1, log_target=True, model=\"elastic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(datasets, pca=False, normalize=True,\n",
    "               n_components=5, C=1, tol=0.01):\n",
    "    x_train, y_train = datasets.get(\"train\")\n",
    "    x_val, y_val = datasets.get(\"val\")\n",
    "    x_test, y_test = datasets.get(\"test\")\n",
    "    \n",
    "    \n",
    "    if pca:\n",
    "        _pca = PCA(n_components=n_components)\n",
    "        _pca.fit(x_train)\n",
    "        x_train = _pca.transform(x_train)\n",
    "        x_val = _pca.transform(x_val)\n",
    "        x_test = _pca.transform(x_test)\n",
    "    if normalize:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_val = scaler.transform(x_val)\n",
    "        x_test = scaler.transform(x_test)  \n",
    "    \n",
    "    \n",
    "    clf = LogisticRegression(penalty='l2',C=C, tol=tol, max_iter=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Accuracy calculation\n",
    "    acc_train = clf.score(x_train, y_train)*100\n",
    "    acc_val = clf.score(x_val, y_val)*100\n",
    "    acc_test = clf.score(x_test, y_test)*100\n",
    "    \n",
    "    print(f\"Accuracy batch 3 (test (secondary)):  {acc_test}%\" )\n",
    "    print(f\"Accuracy (Train):, {acc_train}%\")\n",
    "    print(f\"Accuracy (validation (primary) test): {acc_val}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full\n",
      "Accuracy batch 3 (test (secondary)):  92.5%\n",
      "Accuracy (Train):, 97.5609756097561%\n",
      "Accuracy (validation (primary) test): 90.47619047619048%\n",
      "\n",
      "Variance\n",
      "Accuracy batch 3 (test (secondary)):  95.0%\n",
      "Accuracy (Train):, 78.04878048780488%\n",
      "Accuracy (validation (primary) test): 71.42857142857143%\n",
      "\n",
      "Discharge\n",
      "Accuracy batch 3 (test (secondary)):  97.5%\n",
      "Accuracy (Train):, 92.6829268292683%\n",
      "Accuracy (validation (primary) test): 88.09523809523809%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "print (\"Full\")\n",
    "features_cls = train_val_split(features_df, regression_type=\"full\", model=\"classification\")\n",
    "classification(features_cls, C=9, tol=0.01)\n",
    "\n",
    "print (\"\\nVariance\")\n",
    "features_cls = train_val_split(features_df, regression_type=\"variance\", model=\"classification\")\n",
    "classification(features_cls, C=0.02, tol=0.001)\n",
    "\n",
    "print (\"\\nDischarge\")\n",
    "features_cls = train_val_split(features_df, regression_type=\"discharge\", model=\"classification\")\n",
    "classification(features_cls, C=1, tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
